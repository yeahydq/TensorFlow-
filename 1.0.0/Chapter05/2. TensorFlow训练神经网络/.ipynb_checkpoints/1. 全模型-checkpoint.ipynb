{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.设置输入和输出节点的个数,配置神经网络的参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_NODE = 784     # 输入节点\n",
    "OUTPUT_NODE = 10     # 输出节点\n",
    "LAYER1_NODE = 500    # 隐藏层数       \n",
    "                              \n",
    "BATCH_SIZE = 100     # 每次batch打包的样本个数        \n",
    "\n",
    "# 模型相关的参数\n",
    "LEARNING_RATE_BASE = 0.8      \n",
    "LEARNING_RATE_DECAY = 0.99    \n",
    "REGULARAZTION_RATE = 0.0001   \n",
    "TRAINING_STEPS = 5000        \n",
    "MOVING_AVERAGE_DECAY = 0.99  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 定义辅助函数来计算前向传播结果，使用ReLU做为激活函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(input_tensor, avg_class, weights1, biases1, weights2, biases2):\n",
    "    # 不使用滑动平均类\n",
    "    if avg_class == None:\n",
    "        layer1 = tf.nn.relu(tf.matmul(input_tensor, weights1) + biases1)\n",
    "        return tf.matmul(layer1, weights2) + biases2\n",
    "\n",
    "    else:\n",
    "        # 使用滑动平均类\n",
    "        layer1 = tf.nn.relu(tf.matmul(input_tensor, avg_class.average(weights1)) + avg_class.average(biases1))\n",
    "        return tf.matmul(layer1, avg_class.average(weights2)) + avg_class.average(biases2)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. 定义训练过程。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(mnist):\n",
    "    x = tf.placeholder(tf.float32, [None, INPUT_NODE], name='x-input')\n",
    "    y_ = tf.placeholder(tf.float32, [None, OUTPUT_NODE], name='y-input')\n",
    "    # 生成隐藏层的参数。\n",
    "    weights1 = tf.Variable(tf.truncated_normal([INPUT_NODE, LAYER1_NODE], stddev=0.1))\n",
    "    biases1 = tf.Variable(tf.constant(0.1, shape=[LAYER1_NODE]))\n",
    "    # 生成输出层的参数。\n",
    "    weights2 = tf.Variable(tf.truncated_normal([LAYER1_NODE, OUTPUT_NODE], stddev=0.1))\n",
    "    biases2 = tf.Variable(tf.constant(0.1, shape=[OUTPUT_NODE]))\n",
    "\n",
    "    # 计算不含滑动平均类的前向传播结果\n",
    "    y = inference(x, None, weights1, biases1, weights2, biases2)\n",
    "    \n",
    "    # 定义训练轮数及相关的滑动平均类 \n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    variable_averages = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY, global_step)\n",
    "    # variables_averages_op 跟global_step做绑定了。\n",
    "    # site-packages/tensorflow/python/training/moving_averages.py\n",
    "    # decay = math_ops.minimum(decay,(1.0 + global_step) / (10.0 + global_step))\n",
    "    # 从0.1一直增长到 LEARNING_RATE_DECAY (0.99)\n",
    "    variables_averages_op = variable_averages.apply(tf.trainable_variables())\n",
    "    average_y = inference(x, variable_averages, weights1, biases1, weights2, biases2)\n",
    "    \n",
    "    # 计算交叉熵及其平均值\n",
    "    # 计算交叉熵作为刻画预测值和真实值之间差距的损失函数。这里使用了TensorFlow中提\n",
    "    # 供的sparse_softmax_cross_entropy_with_logits函数来计算交叉熵。当分类\n",
    "    # 问题只有一个正确答案时，可以使用这个函数来加速交叉熵的计算。MNIST问题的图片中\n",
    "    # 只包含了0~9中的一个数字，所以可以使用这个函数来计算交叉熵损失。这个函数的第一个\n",
    "    # 参数是神经网络不包括softmax层的前向传播结果，第二个是训练数据的正确答案。因为\n",
    "    # 标准答案是一个长度位10的一维数组，而该函数需要提供的是一个正确答案的数字，所以需\n",
    "    # 要使用tf.argmax函数来得到正确答案对应的类别编号。\n",
    "    # 注意这里用的是y来计算交叉熵而不是average_y\n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y, labels=tf.argmax(y_, 1))\n",
    "#     cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=average_y, labels=tf.argmax(y_, 1))\n",
    "    cross_entropy_mean = tf.reduce_mean(cross_entropy)\n",
    "    \n",
    "    # 损失函数的计算\n",
    "    regularizer = tf.contrib.layers.l2_regularizer(REGULARAZTION_RATE)\n",
    "    regularaztion = regularizer(weights1) + regularizer(weights2)\n",
    "    loss = cross_entropy_mean + regularaztion\n",
    "    \n",
    "    # 设置指数衰减的学习率。\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        LEARNING_RATE_BASE,\n",
    "        global_step,\n",
    "        mnist.train.num_examples / BATCH_SIZE,\n",
    "        LEARNING_RATE_DECAY,\n",
    "        staircase=True)\n",
    "    \n",
    "    # 优化损失函数\n",
    "    # 在这个函数中，每次执行global_step都会加一。注意这个函数优化的损失函数跟y有关，\n",
    "    # 跟average_y无关。 \n",
    "    # order: loss -> train -> EMA 是train完后的优化\n",
    "    # Since EMA is a post-training(after train) processing step, the optimizer is never exposed to the EMA variables. As such:\n",
    "    #a) the EMA variables are not used to calculate the training sample logits, and therefore\n",
    "    #b) the EMA variables are never exposed to the gradient operations\n",
    "    train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "\n",
    "    # 在训练神经网络模型时，每过一遍数据既需要通过反向传播来更新神经网络中的参数，\n",
    "    # 又要更新每个参数的滑动平均值。为了一次完成多个操作，TensorFlow提供了\n",
    "    # tf.control_dependencies和tf.group两种机制。下面两行程序和\n",
    "    # train_op = tf.group(train_step, variables_average_op)是等价的。\n",
    "    with tf.control_dependencies([train_step, variable_averages_op]):\n",
    "        train_op = tf.no_op(name='train') # tf.no_op是一个没有实际意义的函数\n",
    "        \n",
    "    # 计算正确率 \n",
    "    correct_prediction = tf.equal(tf.argmax(average_y, 1), tf.argmax(y_, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    # 初始化回话并开始训练过程。\n",
    "    with tf.Session() as sess:\n",
    "        tf.global_variables_initializer().run()\n",
    "        validate_feed = {x: mnist.validation.images, y_: mnist.validation.labels}\n",
    "        test_feed = {x: mnist.test.images, y_: mnist.test.labels} \n",
    "        \n",
    "        # 循环的训练神经网络。\n",
    "        for i in range(TRAINING_STEPS):\n",
    "            if i % 1000 == 0:\n",
    "                validate_acc = sess.run(accuracy, feed_dict=validate_feed)\n",
    "                print(\"After %d training step(s), validation accuracy using average model is %g \" % (i, validate_acc))\n",
    "            \n",
    "            xs,ys=mnist.train.next_batch(BATCH_SIZE)\n",
    "            sess.run(train_op,feed_dict={x:xs,y_:ys})\n",
    "\n",
    "        test_acc=sess.run(accuracy,feed_dict=test_feed)\n",
    "        print((\"After %d training step(s), test accuracy using average model is %g\" %(TRAINING_STEPS, test_acc)))\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. 主程序入口，这里设定模型训练次数为5000次。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ~/datasets/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting ~/datasets/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting ~/datasets/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ~/datasets/MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "After 0 training step(s), validation accuracy using average model is 0.0964 \n",
      "After 1000 training step(s), validation accuracy using average model is 0.9772 \n",
      "After 2000 training step(s), validation accuracy using average model is 0.9806 \n",
      "After 3000 training step(s), validation accuracy using average model is 0.9816 \n",
      "After 4000 training step(s), validation accuracy using average model is 0.9812 \n",
      "After 5000 training step(s), test accuracy using average model is 0.983\n"
     ]
    }
   ],
   "source": [
    "def main(argv=None):\n",
    "    mnist = input_data.read_data_sets(\"~/datasets/MNIST_data\", one_hot=True)\n",
    "    train(mnist)\n",
    "\n",
    "if __name__=='__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
